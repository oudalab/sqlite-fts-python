{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokenizer=WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Arma virumque cano, Troiae qui primus ab oris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arma', 'que', 'virum', 'cano', ',', 'Troiae', 'qui', 'primus', 'ab', 'oris']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import sqlite3\n",
    "import ctypes\n",
    "import struct\n",
    "import re\n",
    "import sqlitefts as fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer(fts.Tokenizer):\n",
    "    _p = re.compile(r'\\w+', re.UNICODE)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        for m in self._p.finditer(text):\n",
    "            s, e = m.span()\n",
    "            t = text[s:e]\n",
    "            l = len(t.encode('utf-8'))\n",
    "            p = len(text[:s].encode('utf-8'))\n",
    "            yield t, p, p + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_make_tokenizer():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    tokenizer_module = fts.make_tokenizer_module(SimpleTokenizer())\n",
    "    assert fts.tokenizer.sqlite3_tokenizer_module == type(tokenizer_module)\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#latin tokenizer from cltk that has been updated to be compatible with the sql-fts-wrapper.\n",
    "class WordTokenizer(fts.Tokenizer):  # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Tokenize according to rules specific to a given language.\"\"\"\n",
    "\n",
    "    def __init__(self, language):\n",
    "        \"\"\"Take language as argument to the class. Check availability and\n",
    "        setup class variables.\"\"\"\n",
    "        self.language = language\n",
    "        self.available_languages = ['latin']\n",
    "        assert self.language in self.available_languages, \\\n",
    "            \"Specific tokenizer not available for '{0}'. Only available for: '{1}'.\".format(self.language,  # pylint: disable=line-too-long\n",
    "                                                                                            self.available_languages)  # pylint: disable=line-too-long\n",
    "\n",
    "        if self.language == 'latin':\n",
    "            self.enclitics = ['que', 'ne', 'ue', 've', 'cum','st']\n",
    "\n",
    "            self.inclusions = []\n",
    "            \n",
    "            cum_inclusions = ['mecum', 'tecum', 'secum', 'nobiscum', 'vobiscum', 'quocum', 'quicum' 'quibuscum']\n",
    "            \n",
    "            self.exceptions = self.enclitics\n",
    "\n",
    "            que_exceptions = []\n",
    "            ne_exceptions = []\n",
    "            ue_exceptions = []\n",
    "            ve_exceptions = []\n",
    "            cum_exceptions = []\n",
    "            st_exceptions = []\n",
    "\n",
    "            # quisque\n",
    "            que_exceptions += ['quisque', 'quidque', 'quicque', 'quodque', 'cuiusque', 'cuique',\n",
    "                               'quemque', 'quoque', 'quique', 'quaeque', 'quorumque', 'quarumque',\n",
    "                               'quibusque', 'quosque', 'quasque']\n",
    "\n",
    "            # uterque\n",
    "            que_exceptions += ['uterque', 'utraque', 'utrumque', 'utriusque', 'utrique', 'utrumque',\n",
    "                               'utramque', 'utroque', 'utraque', 'utrique', 'utraeque', 'utrorumque',\n",
    "                               'utrarumque', 'utrisque', 'utrosque', 'utrasque']\n",
    "\n",
    "            # quiscumque\n",
    "            que_exceptions += ['quicumque', 'quidcumque', 'quodcumque', 'cuiuscumque', 'cuicumque',\n",
    "                               'quemcumque', 'quamcumque', 'quocumque', 'quacumque', 'quicumque',\n",
    "                               'quaecumque', 'quorumcumque', 'quarumcumque', 'quibuscumque',\n",
    "                               'quoscumque', 'quascumque']\n",
    "\n",
    "            # unuscumque\n",
    "            que_exceptions += ['unusquisque', 'unaquaeque', 'unumquodque', 'unumquidque',\n",
    "                               'uniuscuiusque', 'unicuique', 'unumquemque', 'unamquamque', 'unoquoque',\n",
    "                               'unaquaque']\n",
    "\n",
    "            # plerusque\n",
    "            que_exceptions += ['plerusque', 'pleraque', 'plerumque', 'plerique', 'pleraeque',\n",
    "                               'pleroque', 'pleramque', 'plerorumque', 'plerarumque', 'plerisque',\n",
    "                               'plerosque', 'plerasque']\n",
    "\n",
    "            # misc\n",
    "            que_exceptions += ['absque', 'abusque', 'adaeque', 'adusque', 'aeque', 'antique', 'atque',\n",
    "                               'circumundique', 'conseque', 'cumque', 'cunque', 'denique', 'deque',\n",
    "                               'donique', 'hucusque', 'inique', 'inseque', 'itaque', 'longinque',\n",
    "                               'namque', 'oblique', 'peraeque', 'praecoque', 'propinque',\n",
    "                               'qualiscumque', 'quandocumque', 'quandoque', 'quantuluscumque',\n",
    "                               'quantumcumque', 'quantuscumque', 'quinque', 'quocumque',\n",
    "                               'quomodocumque', 'quomque', 'quotacumque', 'quotcumque',\n",
    "                               'quotienscumque', 'quotiensque', 'quotusquisque', 'quousque', 'relinque',\n",
    "                               'simulatque', 'torque', 'ubicumque', 'ubique', 'undecumque', 'undique',\n",
    "                               'usque', 'usquequaque', 'utcumque', 'utercumque', 'utique', 'utrimque',\n",
    "                               'utrique', 'utriusque', 'utrobique', 'utrubique']\n",
    "\n",
    "            ne_exceptions += ['absone', 'acharne', 'acrisione', 'acumine', 'adhucine', 'adsuetudine',\n",
    "                              'aeetine', 'aeschynomene', 'aesone', 'agamemnone', 'agmine', 'albane',\n",
    "                              'alcyone', 'almone', 'alsine', 'amasene', 'ambitione', 'amne', 'amoene',\n",
    "                              'amymone', 'anadyomene', 'andrachne', 'anemone', 'aniene', 'anne',\n",
    "                              'antigone', 'aparine', 'apolline', 'aquilone', 'arachne', 'arne',\n",
    "                              'arundine', 'ascanione', 'asiane', 'asine', 'aspargine', 'babylone',\n",
    "                              'barine', 'bellone', 'belone', 'bene', 'benigne', 'bipenne', 'bizone',\n",
    "                              'bone', 'bubone', 'bulbine', 'cacumine', 'caligine', 'calymne', 'cane',\n",
    "                              'carcine', 'cardine', 'carmine', 'catacecaumene', 'catone', 'cerne',\n",
    "                              'certamine', 'chalbane', 'chamaedaphne', 'chamaemyrsine', 'chaone',\n",
    "                              'chione', 'christiane', 'clymene', 'cognomine', 'commagene', 'commune',\n",
    "                              'compone', 'concinne', 'condicione', 'condigne', 'cone', 'confine',\n",
    "                              'consone', 'corone', 'crastine', 'crepidine', 'crimine', 'crine',\n",
    "                              'culmine', 'cupidine', 'cyane', 'cydne', 'cyllene', 'cyrene', 'daphne',\n",
    "                              'depone', 'desine', 'dicione', 'digne', 'dine', 'dione', 'discrimine',\n",
    "                              'diutine', 'dracone', 'dulcedine', 'elatine', 'elephantine', 'elleborine',\n",
    "                              'epidamne', 'erigone', 'euadne', 'euphrone', 'euphrosyne', 'examine',\n",
    "                              'faune', 'femine', 'feminine', 'ferrugine', 'fine', 'flamine', 'flumine',\n",
    "                              'formidine', 'fragmine', 'fraterne', 'fulmine', 'fune', 'germane',\n",
    "                              'germine', 'geryone', 'gorgone', 'gramine', 'grandine', 'haecine',\n",
    "                              'halcyone', 'hammone', 'harundine', 'hedone', 'helene', 'helxine',\n",
    "                              'hermione', 'heroine', 'hesione', 'hicine', 'hicne', 'hierabotane',\n",
    "                              'hippocrene', 'hispane', 'hodierne', 'homine', 'hominesne', 'hortamine',\n",
    "                              'hucine', 'humane', 'hunccine', 'huncine', 'iasione', 'iasone', 'igne',\n",
    "                              'imagine', 'immane', 'immune', 'impoene', 'impone', 'importune', 'impune',\n",
    "                              'inane', 'inconcinne', 'indagine', 'indigne', 'inferne', 'inguine',\n",
    "                              'inhumane', 'inpone', 'inpune', 'insane', 'insigne', 'inurbane', 'ismene',\n",
    "                              'istucine', 'itone', 'iuuene', 'karthagine', 'labiene', 'lacedaemone',\n",
    "                              'lanugine', 'latine', 'legione', 'lene', 'lenone', 'libidine', 'limine',\n",
    "                              'limone', 'lumine', 'magne', 'maligne', 'mane', 'margine', 'marone',\n",
    "                              'masculine', 'matutine', 'medicamine', 'melpomene', 'memnone', 'mesene',\n",
    "                              'messene', 'misene', 'mitylene', 'mnemosyne', 'moderamine', 'moene',\n",
    "                              'mone', 'mortaline', 'mucrone', 'munimine', 'myrmidone', 'mytilene',\n",
    "                              'necne', 'neptune', 'nequene', 'nerine', 'nocturne', 'nomine', 'nonne',\n",
    "                              'nullane', 'numine', 'nuncine', 'nyctimene', 'obscene', 'obsidione',\n",
    "                              'oenone', 'omine', 'omne', 'oppone', 'opportune', 'ordine', 'origine',\n",
    "                              'orphne', 'oxymyrsine', 'paene', 'pallene', 'pane', 'paraetacene',\n",
    "                              'patalene', 'pectine', 'pelagine', 'pellene', 'pene', 'perbene',\n",
    "                              'perbenigne', 'peremne', 'perenne', 'perindigne', 'peropportune',\n",
    "                              'persephone', 'phryne', 'pirene', 'pitane', 'plane', 'pleione', 'plene',\n",
    "                              'pone', 'praefiscine', 'prasiane', 'priene', 'priuigne', 'procne',\n",
    "                              'proditione', 'progne', 'prone', 'propone', 'pulmone', 'pylene', 'pyrene',\n",
    "                              'pythone', 'ratione', 'regione', 'religione', 'remane', 'retine', 'rhene',\n",
    "                              'rhododaphne', 'robigine', 'romane', 'roxane', 'rubigine', 'sabine',\n",
    "                              'sane', 'sanguine', 'saturne', 'seditione', 'segne', 'selene', 'semine',\n",
    "                              'semiplene', 'sene', 'sepone', 'serene', 'sermone', 'serrane', 'siccine',\n",
    "                              'sicine', 'sine', 'sithone', 'solane', 'sollemne', 'somne', 'sophene',\n",
    "                              'sperne', 'spiramine', 'stamine', 'statione', 'stephane', 'sterne',\n",
    "                              'stramine', 'subpone', 'subtegmine', 'subtemine', 'sulmone', 'superne',\n",
    "                              'supine', 'suppone', 'susiane', 'syene', 'tantane', 'tantine', 'taprobane',\n",
    "                              'tegmine', 'telamone', 'temne', 'temone', 'tene', 'testudine', 'theophane',\n",
    "                              'therone', 'thyone', 'tiberine', 'tibicine', 'tiburne', 'tirone',\n",
    "                              'tisiphone', 'torone', 'transitione', 'troiane', 'turbine', 'turne',\n",
    "                              'tyrrhene', 'uane', 'uelamine', 'uertigine', 'uesane', 'uimine', 'uirgine',\n",
    "                              'umbone', 'unguine', 'uolumine', 'uoragine', 'urbane', 'uulcane', 'zone']\n",
    "\n",
    "            ue_exceptions += ['agaue', 'ambigue', 'assidue', 'aue', 'boue', 'breue', 'calue', 'caue',\n",
    "                              'ciue', 'congrue', 'contigue', 'continue', 'curue', 'exigue', 'exue',\n",
    "                              'fatue', 'faue', 'fue', 'furtiue', 'gradiue', 'graue', 'ignaue',\n",
    "                              'incongrue', 'ingenue', 'innocue', 'ioue', 'lasciue', 'leue', 'moue',\n",
    "                              'mutue', 'naue', 'neue', 'niue', 'perexigue', 'perspicue', 'pingue',\n",
    "                              'praecipue', 'praegraue', 'prospicue', 'proterue', 'remoue', 'resolue',\n",
    "                              'saeue', 'salue', 'siue', 'solue', 'strenue', 'sue', 'summoue',\n",
    "                              'superflue', 'supplicue', 'tenue', 'uiue', 'ungue', 'uoue']\n",
    "\n",
    "            ve_exceptions += ['agave', 'ave', 'bove', 'breve', 'calve', 'cave', 'cive', 'curve', 'fave',\n",
    "                              'furtive', 'gradive', 'grave', 'ignave', 'iove', 'lascive', 'leve', 'move',\n",
    "                              'nave', 'neve', 'nive', 'praegrave', 'prospicve', 'proterve', 'remove',\n",
    "                              'resolve', 'saeve', 'salve', 'sive', 'solve', 'summove', 'vive', 'vove']\n",
    "\n",
    "            st_exceptions += ['abest', 'adest', 'ast', 'deest', 'est', 'inest', 'interest', 'post', 'potest', 'prodest', 'subest', 'superest']\n",
    "\n",
    "            self.exceptions = list(set(self.exceptions\n",
    "                                       + que_exceptions\n",
    "                                       + ne_exceptions\n",
    "                                       + ue_exceptions\n",
    "                                       + ve_exceptions\n",
    "                                       + st_exceptions\n",
    "                                       ))\n",
    "\n",
    "            self.inclusions = list(set(self.inclusions\n",
    "                                       + cum_inclusions))\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        \"\"\"Tokenize incoming string.\"\"\"\n",
    "        punkt = PunktLanguageVars()\n",
    "        generic_tokens = punkt.word_tokenize(string)\n",
    "        generic_tokens = [x for item in generic_tokens for x in ([item] if item != 'nec' else ['c', 'ne'])] # Handle 'nec' as a special case.\n",
    "        specific_tokens = []\n",
    "        for generic_token in generic_tokens:\n",
    "            is_enclitic = False\n",
    "            if generic_token not in self.exceptions:\n",
    "                for enclitic in self.enclitics:\n",
    "                    if generic_token.endswith(enclitic):\n",
    "                        if enclitic == 'cum':\n",
    "                            if generic_token in self.inclusions:\n",
    "                                specific_tokens += [enclitic] + [generic_token[:-len(enclitic)]]\n",
    "                            else:\n",
    "                                specific_tokens += [generic_token]                                                                         \n",
    "                        elif enclitic == 'st':\n",
    "                            if generic_token.endswith('ust'):\n",
    "                                specific_tokens += [generic_token[:-len(enclitic)+1]] + ['est']\n",
    "                            else:\n",
    "                                # Does not handle 'similist', 'qualist', etc. correctly\n",
    "                                specific_tokens += [generic_token[:-len(enclitic)]] + ['est']\n",
    "                        else:\n",
    "                            specific_tokens += [enclitic] + [generic_token[:-len(enclitic)]]\n",
    "                        is_enclitic = True\n",
    "                        break\n",
    "            if not is_enclitic:\n",
    "                specific_tokens.append(generic_token)\n",
    "        #return iter(specific_tokens) #change this one into an iterator.\n",
    "        startPoint=0 #this is to accumulate the start point.\n",
    "        for item in specific_tokens:\n",
    "            itemLength=len(item)\n",
    "            yield item, startPoint, startPoint+itemLength\n",
    "            startPoint=startPoint+itemLength+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latin_token=WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latinResult=latin_token.tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Arma', 0, 4)\n",
      "('que', 5, 8)\n",
      "('virum', 9, 14)\n",
      "('cano', 15, 19)\n",
      "(',', 20, 21)\n",
      "('Troiae', 22, 28)\n",
      "('qui', 29, 32)\n",
      "('primus', 33, 39)\n",
      "('ab', 40, 42)\n",
      "('ori', 43, 46)\n",
      "('c', 47, 48)\n",
      "('ne', 49, 51)\n"
     ]
    }
   ],
   "source": [
    "for item in latinResult:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_make_tokenizer():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    tokenizer_module = fts.make_tokenizer_module(WordTokenizer('latin'))\n",
    "    assert fts.tokenizer.sqlite3_tokenizer_module == type(tokenizer_module)\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_make_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_reginster_tokenizer():\n",
    "    name = 'simpe'\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    tokenizer_module = fts.make_tokenizer_module(WordTokenizer('latin'))\n",
    "    fts.register_tokenizer(c, name, tokenizer_module)\n",
    "    v = c.execute(\"SELECT FTS3_TOKENIZER(?)\", (name,)).fetchone()[0]\n",
    "    assert ctypes.addressof(tokenizer_module) == struct.unpack(\"P\", v)[0]\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_reginster_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_createtable():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    c.row_factory = sqlite3.Row\n",
    "    name = 'simple'\n",
    "    sql = \"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\".format(name)\n",
    "    fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "    c.execute(sql)\n",
    "    r = c.execute(\"SELECT * FROM sqlite_master WHERE type='table' AND name='fts'\").fetchone()\n",
    "    assert r\n",
    "    assert r[str('type')] == 'table' and r[str('name')] == 'fts' and r[str('tbl_name')] == 'fts'\n",
    "    assert r[str('sql')].upper() == sql.upper()\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_createtable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktLanguageVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_insert():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    c.row_factory = sqlite3.Row\n",
    "    name = 'simple'\n",
    "    content = 'Arma virumque cano, Troiae qui primus ab ori'\n",
    "    fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "    c.execute(\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\".format(name))\n",
    "    r = c.execute('INSERT INTO fts VALUES(?)', (content,))\n",
    "    assert r.rowcount == 1\n",
    "    r = c.execute(\"SELECT * FROM fts\").fetchone()\n",
    "    assert r\n",
    "    assert r[str('content')] == content\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_match():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    c.row_factory = sqlite3.Row\n",
    "    name = 'simple'\n",
    "    contents = [('ibi linguam Atthidem primis pueritiae stipendiis merui',),\n",
    "                ('ibi linguam haec equidem ipsa vocis immutatio',)\n",
    "               ]\n",
    "    fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "    c.execute(\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\".format(name))\n",
    "    r = c.executemany('INSERT INTO fts VALUES(?)', contents)\n",
    "    assert r.rowcount == 2\n",
    "    r = c.execute(\"SELECT * FROM fts\").fetchall()\n",
    "    assert len(r) == 2\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'ibi'\").fetchall()\n",
    "    assert len(r) == 2\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'Atthidem'\").fetchall()\n",
    "    assert len(r) == 1 and r[0][str('content')] == contents[0][0]\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'haec'\").fetchall()\n",
    "    assert len(r) == 1 and r[0][str('content')] == contents[1][0]\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'zzz'\").fetchall()\n",
    "    assert len(r) == 0\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_full_text_index_queries():\n",
    "    name = 'simple'\n",
    "    docs = [('README', 'huius commentarii pertinebit fortassis et ad successorem utilitas, sed cum inter initia administrationis meae scriptus sit,'\n",
    "             +'in primis ad meam institutionem regulamque proficie'),\n",
    "            ('LICENSE', ''' Cum omnis res ab imperatore delegata intentiorem exigat curam,\n",
    "            et me seu naturalis sollicitudo seu fides sedula non ad \n",
    "            diligentiam modo verum ad amorem quoque commissae rei instigent sitque nunc mihi ab Nerva Augusto, nescio diligentiore an amantiore rei publicae imperatore, aquarum iniunctum officium ad usum, tum ad salubritatem atque etiam securitatem urbis pertinens, administratum per principes semper civitatis nostrae viros, primum ac potissimum existimo, \n",
    "            sicut in ceteris negotiis institueram, nosse quod suscepi.''')\n",
    "          ]\n",
    "    with sqlite3.connect(':memory:') as c:\n",
    "        c.row_factory = sqlite3.Row\n",
    "        fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "        c.execute(\"CREATE VIRTUAL TABLE docs USING FTS4(title, body, tokenize={})\".format(name))\n",
    "        c.executemany(\"INSERT INTO docs(title, body) VALUES(?, ?)\", docs)\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'huius'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'sed'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'sed*'\").fetchall()\n",
    "        assert len(r) == 2\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'comm'\").fetchall()\n",
    "        assert len(r) == 0\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'commi*'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'comm*'\").fetchall()\n",
    "        assert len(r) == 2\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'quod suscepi'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH '\\\"qu* sus*\\\"'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-d73c5bcdf3f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_full_text_index_queries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-131-e30d7d46a3f1>\u001b[0m in \u001b[0;36mtest_full_text_index_queries\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM docs WHERE docs MATCH 'quod suscepi'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM docs WHERE docs MATCH '\\\"qu* sus*\\\"'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_full_text_index_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
