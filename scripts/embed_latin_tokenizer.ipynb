{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokenizer=WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Arma virumque cano, Troiae qui primus ab oris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arma', 'que', 'virum', 'cano', ',', 'Troiae', 'qui', 'primus', 'ab', 'oris']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import sqlite3\n",
    "import ctypes\n",
    "import struct\n",
    "import re\n",
    "import sqlitefts as fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer(fts.Tokenizer):\n",
    "    _p = re.compile(r'\\w+', re.UNICODE)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        for m in self._p.finditer(text):\n",
    "            s, e = m.span()\n",
    "            t = text[s:e]\n",
    "            l = len(t.encode('utf-8'))\n",
    "            p = len(text[:s].encode('utf-8'))\n",
    "            yield t, p, p + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_make_tokenizer():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    tokenizer_module = fts.make_tokenizer_module(SimpleTokenizer())\n",
    "    assert fts.tokenizer.sqlite3_tokenizer_module == type(tokenizer_module)\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#latin tokenizer from cltk that has been updated to be compatible with the sql-fts-wrapper.\n",
    "class WordTokenizer(fts.Tokenizer):  # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Tokenize according to rules specific to a given language.\"\"\"\n",
    "\n",
    "    def __init__(self, language):\n",
    "        \"\"\"Take language as argument to the class. Check availability and\n",
    "        setup class variables.\"\"\"\n",
    "        self.language = language\n",
    "        self.available_languages = ['latin']\n",
    "        assert self.language in self.available_languages, \\\n",
    "            \"Specific tokenizer not available for '{0}'. Only available for: '{1}'.\".format(self.language,  # pylint: disable=line-too-long\n",
    "                                                                                            self.available_languages)  # pylint: disable=line-too-long\n",
    "\n",
    "        if self.language == 'latin':\n",
    "            self.enclitics = ['que', 'ne', 'ue', 've', 'cum','st']\n",
    "\n",
    "            self.inclusions = []\n",
    "            \n",
    "            cum_inclusions = ['mecum', 'tecum', 'secum', 'nobiscum', 'vobiscum', 'quocum', 'quicum' 'quibuscum']\n",
    "            \n",
    "            self.exceptions = self.enclitics\n",
    "\n",
    "            que_exceptions = []\n",
    "            ne_exceptions = []\n",
    "            ue_exceptions = []\n",
    "            ve_exceptions = []\n",
    "            cum_exceptions = []\n",
    "            st_exceptions = []\n",
    "\n",
    "            # quisque\n",
    "            que_exceptions += ['quisque', 'quidque', 'quicque', 'quodque', 'cuiusque', 'cuique',\n",
    "                               'quemque', 'quoque', 'quique', 'quaeque', 'quorumque', 'quarumque',\n",
    "                               'quibusque', 'quosque', 'quasque']\n",
    "\n",
    "            # uterque\n",
    "            que_exceptions += ['uterque', 'utraque', 'utrumque', 'utriusque', 'utrique', 'utrumque',\n",
    "                               'utramque', 'utroque', 'utraque', 'utrique', 'utraeque', 'utrorumque',\n",
    "                               'utrarumque', 'utrisque', 'utrosque', 'utrasque']\n",
    "\n",
    "            # quiscumque\n",
    "            que_exceptions += ['quicumque', 'quidcumque', 'quodcumque', 'cuiuscumque', 'cuicumque',\n",
    "                               'quemcumque', 'quamcumque', 'quocumque', 'quacumque', 'quicumque',\n",
    "                               'quaecumque', 'quorumcumque', 'quarumcumque', 'quibuscumque',\n",
    "                               'quoscumque', 'quascumque']\n",
    "\n",
    "            # unuscumque\n",
    "            que_exceptions += ['unusquisque', 'unaquaeque', 'unumquodque', 'unumquidque',\n",
    "                               'uniuscuiusque', 'unicuique', 'unumquemque', 'unamquamque', 'unoquoque',\n",
    "                               'unaquaque']\n",
    "\n",
    "            # plerusque\n",
    "            que_exceptions += ['plerusque', 'pleraque', 'plerumque', 'plerique', 'pleraeque',\n",
    "                               'pleroque', 'pleramque', 'plerorumque', 'plerarumque', 'plerisque',\n",
    "                               'plerosque', 'plerasque']\n",
    "\n",
    "            # misc\n",
    "            que_exceptions += ['absque', 'abusque', 'adaeque', 'adusque', 'aeque', 'antique', 'atque',\n",
    "                               'circumundique', 'conseque', 'cumque', 'cunque', 'denique', 'deque',\n",
    "                               'donique', 'hucusque', 'inique', 'inseque', 'itaque', 'longinque',\n",
    "                               'namque', 'oblique', 'peraeque', 'praecoque', 'propinque',\n",
    "                               'qualiscumque', 'quandocumque', 'quandoque', 'quantuluscumque',\n",
    "                               'quantumcumque', 'quantuscumque', 'quinque', 'quocumque',\n",
    "                               'quomodocumque', 'quomque', 'quotacumque', 'quotcumque',\n",
    "                               'quotienscumque', 'quotiensque', 'quotusquisque', 'quousque', 'relinque',\n",
    "                               'simulatque', 'torque', 'ubicumque', 'ubique', 'undecumque', 'undique',\n",
    "                               'usque', 'usquequaque', 'utcumque', 'utercumque', 'utique', 'utrimque',\n",
    "                               'utrique', 'utriusque', 'utrobique', 'utrubique']\n",
    "\n",
    "            ne_exceptions += ['absone', 'acharne', 'acrisione', 'acumine', 'adhucine', 'adsuetudine',\n",
    "                              'aeetine', 'aeschynomene', 'aesone', 'agamemnone', 'agmine', 'albane',\n",
    "                              'alcyone', 'almone', 'alsine', 'amasene', 'ambitione', 'amne', 'amoene',\n",
    "                              'amymone', 'anadyomene', 'andrachne', 'anemone', 'aniene', 'anne',\n",
    "                              'antigone', 'aparine', 'apolline', 'aquilone', 'arachne', 'arne',\n",
    "                              'arundine', 'ascanione', 'asiane', 'asine', 'aspargine', 'babylone',\n",
    "                              'barine', 'bellone', 'belone', 'bene', 'benigne', 'bipenne', 'bizone',\n",
    "                              'bone', 'bubone', 'bulbine', 'cacumine', 'caligine', 'calymne', 'cane',\n",
    "                              'carcine', 'cardine', 'carmine', 'catacecaumene', 'catone', 'cerne',\n",
    "                              'certamine', 'chalbane', 'chamaedaphne', 'chamaemyrsine', 'chaone',\n",
    "                              'chione', 'christiane', 'clymene', 'cognomine', 'commagene', 'commune',\n",
    "                              'compone', 'concinne', 'condicione', 'condigne', 'cone', 'confine',\n",
    "                              'consone', 'corone', 'crastine', 'crepidine', 'crimine', 'crine',\n",
    "                              'culmine', 'cupidine', 'cyane', 'cydne', 'cyllene', 'cyrene', 'daphne',\n",
    "                              'depone', 'desine', 'dicione', 'digne', 'dine', 'dione', 'discrimine',\n",
    "                              'diutine', 'dracone', 'dulcedine', 'elatine', 'elephantine', 'elleborine',\n",
    "                              'epidamne', 'erigone', 'euadne', 'euphrone', 'euphrosyne', 'examine',\n",
    "                              'faune', 'femine', 'feminine', 'ferrugine', 'fine', 'flamine', 'flumine',\n",
    "                              'formidine', 'fragmine', 'fraterne', 'fulmine', 'fune', 'germane',\n",
    "                              'germine', 'geryone', 'gorgone', 'gramine', 'grandine', 'haecine',\n",
    "                              'halcyone', 'hammone', 'harundine', 'hedone', 'helene', 'helxine',\n",
    "                              'hermione', 'heroine', 'hesione', 'hicine', 'hicne', 'hierabotane',\n",
    "                              'hippocrene', 'hispane', 'hodierne', 'homine', 'hominesne', 'hortamine',\n",
    "                              'hucine', 'humane', 'hunccine', 'huncine', 'iasione', 'iasone', 'igne',\n",
    "                              'imagine', 'immane', 'immune', 'impoene', 'impone', 'importune', 'impune',\n",
    "                              'inane', 'inconcinne', 'indagine', 'indigne', 'inferne', 'inguine',\n",
    "                              'inhumane', 'inpone', 'inpune', 'insane', 'insigne', 'inurbane', 'ismene',\n",
    "                              'istucine', 'itone', 'iuuene', 'karthagine', 'labiene', 'lacedaemone',\n",
    "                              'lanugine', 'latine', 'legione', 'lene', 'lenone', 'libidine', 'limine',\n",
    "                              'limone', 'lumine', 'magne', 'maligne', 'mane', 'margine', 'marone',\n",
    "                              'masculine', 'matutine', 'medicamine', 'melpomene', 'memnone', 'mesene',\n",
    "                              'messene', 'misene', 'mitylene', 'mnemosyne', 'moderamine', 'moene',\n",
    "                              'mone', 'mortaline', 'mucrone', 'munimine', 'myrmidone', 'mytilene',\n",
    "                              'necne', 'neptune', 'nequene', 'nerine', 'nocturne', 'nomine', 'nonne',\n",
    "                              'nullane', 'numine', 'nuncine', 'nyctimene', 'obscene', 'obsidione',\n",
    "                              'oenone', 'omine', 'omne', 'oppone', 'opportune', 'ordine', 'origine',\n",
    "                              'orphne', 'oxymyrsine', 'paene', 'pallene', 'pane', 'paraetacene',\n",
    "                              'patalene', 'pectine', 'pelagine', 'pellene', 'pene', 'perbene',\n",
    "                              'perbenigne', 'peremne', 'perenne', 'perindigne', 'peropportune',\n",
    "                              'persephone', 'phryne', 'pirene', 'pitane', 'plane', 'pleione', 'plene',\n",
    "                              'pone', 'praefiscine', 'prasiane', 'priene', 'priuigne', 'procne',\n",
    "                              'proditione', 'progne', 'prone', 'propone', 'pulmone', 'pylene', 'pyrene',\n",
    "                              'pythone', 'ratione', 'regione', 'religione', 'remane', 'retine', 'rhene',\n",
    "                              'rhododaphne', 'robigine', 'romane', 'roxane', 'rubigine', 'sabine',\n",
    "                              'sane', 'sanguine', 'saturne', 'seditione', 'segne', 'selene', 'semine',\n",
    "                              'semiplene', 'sene', 'sepone', 'serene', 'sermone', 'serrane', 'siccine',\n",
    "                              'sicine', 'sine', 'sithone', 'solane', 'sollemne', 'somne', 'sophene',\n",
    "                              'sperne', 'spiramine', 'stamine', 'statione', 'stephane', 'sterne',\n",
    "                              'stramine', 'subpone', 'subtegmine', 'subtemine', 'sulmone', 'superne',\n",
    "                              'supine', 'suppone', 'susiane', 'syene', 'tantane', 'tantine', 'taprobane',\n",
    "                              'tegmine', 'telamone', 'temne', 'temone', 'tene', 'testudine', 'theophane',\n",
    "                              'therone', 'thyone', 'tiberine', 'tibicine', 'tiburne', 'tirone',\n",
    "                              'tisiphone', 'torone', 'transitione', 'troiane', 'turbine', 'turne',\n",
    "                              'tyrrhene', 'uane', 'uelamine', 'uertigine', 'uesane', 'uimine', 'uirgine',\n",
    "                              'umbone', 'unguine', 'uolumine', 'uoragine', 'urbane', 'uulcane', 'zone']\n",
    "\n",
    "            ue_exceptions += ['agaue', 'ambigue', 'assidue', 'aue', 'boue', 'breue', 'calue', 'caue',\n",
    "                              'ciue', 'congrue', 'contigue', 'continue', 'curue', 'exigue', 'exue',\n",
    "                              'fatue', 'faue', 'fue', 'furtiue', 'gradiue', 'graue', 'ignaue',\n",
    "                              'incongrue', 'ingenue', 'innocue', 'ioue', 'lasciue', 'leue', 'moue',\n",
    "                              'mutue', 'naue', 'neue', 'niue', 'perexigue', 'perspicue', 'pingue',\n",
    "                              'praecipue', 'praegraue', 'prospicue', 'proterue', 'remoue', 'resolue',\n",
    "                              'saeue', 'salue', 'siue', 'solue', 'strenue', 'sue', 'summoue',\n",
    "                              'superflue', 'supplicue', 'tenue', 'uiue', 'ungue', 'uoue']\n",
    "\n",
    "            ve_exceptions += ['agave', 'ave', 'bove', 'breve', 'calve', 'cave', 'cive', 'curve', 'fave',\n",
    "                              'furtive', 'gradive', 'grave', 'ignave', 'iove', 'lascive', 'leve', 'move',\n",
    "                              'nave', 'neve', 'nive', 'praegrave', 'prospicve', 'proterve', 'remove',\n",
    "                              'resolve', 'saeve', 'salve', 'sive', 'solve', 'summove', 'vive', 'vove']\n",
    "\n",
    "            st_exceptions += ['abest', 'adest', 'ast', 'deest', 'est', 'inest', 'interest', 'post', 'potest', 'prodest', 'subest', 'superest']\n",
    "\n",
    "            self.exceptions = list(set(self.exceptions\n",
    "                                       + que_exceptions\n",
    "                                       + ne_exceptions\n",
    "                                       + ue_exceptions\n",
    "                                       + ve_exceptions\n",
    "                                       + st_exceptions\n",
    "                                       ))\n",
    "\n",
    "            self.inclusions = list(set(self.inclusions\n",
    "                                       + cum_inclusions))\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        \"\"\"Tokenize incoming string.\"\"\"\n",
    "        punkt = PunktLanguageVars()\n",
    "        generic_tokens = punkt.word_tokenize(string)\n",
    "        generic_tokens = [x for item in generic_tokens for x in ([item] if item != 'nec' else ['c', 'ne'])] # Handle 'nec' as a special case.\n",
    "        specific_tokens = []\n",
    "        for generic_token in generic_tokens:\n",
    "            is_enclitic = False\n",
    "            if generic_token not in self.exceptions:\n",
    "                for enclitic in self.enclitics:\n",
    "                    if generic_token.endswith(enclitic):\n",
    "                        if enclitic == 'cum':\n",
    "                            if generic_token in self.inclusions:\n",
    "                                specific_tokens += [enclitic] + [generic_token[:-len(enclitic)]]\n",
    "                            else:\n",
    "                                specific_tokens += [generic_token]                                                                         \n",
    "                        elif enclitic == 'st':\n",
    "                            if generic_token.endswith('ust'):\n",
    "                                specific_tokens += [generic_token[:-len(enclitic)+1]] + ['est']\n",
    "                            else:\n",
    "                                # Does not handle 'similist', 'qualist', etc. correctly\n",
    "                                specific_tokens += [generic_token[:-len(enclitic)]] + ['est']\n",
    "                        else:\n",
    "                            specific_tokens += [enclitic] + [generic_token[:-len(enclitic)]]\n",
    "                        is_enclitic = True\n",
    "                        break\n",
    "            if not is_enclitic:\n",
    "                specific_tokens.append(generic_token)\n",
    "        #return iter(specific_tokens) #change this one into an iterator.\n",
    "        startPoint=0 #this is to accumulate the start point.\n",
    "        for item in specific_tokens:\n",
    "            itemLength=len(item)\n",
    "            yield item, startPoint, startPoint+itemLength\n",
    "            startPoint=startPoint+itemLength+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latin_token=WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latinResult=latin_token.tokenize(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello', 0, 5)\n"
     ]
    }
   ],
   "source": [
    "for item in latinResult:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cum', 0, 3)\n",
      "('omnis', 4, 9)\n",
      "('res', 10, 13)\n",
      "('ab', 14, 16)\n",
      "('imperatore', 17, 27)\n",
      "('delegata', 28, 36)\n",
      "('intentiorem', 37, 48)\n",
      "('exigat', 49, 55)\n",
      "('curam', 56, 61)\n",
      "(',', 62, 63)\n",
      "('et', 64, 66)\n",
      "('me', 67, 69)\n",
      "('seu', 70, 73)\n",
      "('naturalis', 74, 83)\n",
      "('sollicitudo', 84, 95)\n",
      "('seu', 96, 99)\n",
      "('fides', 100, 105)\n",
      "('sedula', 106, 112)\n",
      "('non', 113, 116)\n",
      "('ad', 117, 119)\n",
      "('diligentiam', 120, 131)\n",
      "('modo', 132, 136)\n",
      "('verum', 137, 142)\n",
      "('ad', 143, 145)\n",
      "('amorem', 146, 152)\n",
      "('quoque', 153, 159)\n",
      "('commissae', 160, 169)\n",
      "('rei', 170, 173)\n",
      "('instigent', 174, 183)\n",
      "('que', 184, 187)\n",
      "('sit', 188, 191)\n",
      "('nunc', 192, 196)\n",
      "('mihi', 197, 201)\n",
      "('ab', 202, 204)\n",
      "('Nerva', 205, 210)\n",
      "('Augusto', 211, 218)\n",
      "(',', 219, 220)\n",
      "('nescio', 221, 227)\n",
      "('diligentiore', 228, 240)\n",
      "('an', 241, 243)\n",
      "('amantiore', 244, 253)\n",
      "('rei', 254, 257)\n",
      "('publicae', 258, 266)\n",
      "('imperatore', 267, 277)\n",
      "(',', 278, 279)\n",
      "('aquarum', 280, 287)\n",
      "('iniunctum', 288, 297)\n",
      "('officium', 298, 306)\n",
      "('ad', 307, 309)\n",
      "('usum', 310, 314)\n",
      "(',', 315, 316)\n",
      "('tum', 317, 320)\n",
      "('ad', 321, 323)\n",
      "('salubritatem', 324, 336)\n",
      "('atque', 337, 342)\n",
      "('etiam', 343, 348)\n",
      "('securitatem', 349, 360)\n",
      "('urbis', 361, 366)\n",
      "('pertinens', 367, 376)\n",
      "(',', 377, 378)\n",
      "('administratum', 379, 392)\n",
      "('per', 393, 396)\n",
      "('principes', 397, 406)\n",
      "('semper', 407, 413)\n",
      "('civitatis', 414, 423)\n",
      "('nostrae', 424, 431)\n",
      "('viros', 432, 437)\n",
      "(',', 438, 439)\n",
      "('primum', 440, 446)\n",
      "('ac', 447, 449)\n",
      "('potissimum', 450, 460)\n",
      "('existimo,sicut', 461, 475)\n",
      "('in', 476, 478)\n",
      "('ceteris', 479, 486)\n",
      "('negotiis', 487, 495)\n",
      "('institueram', 496, 507)\n",
      "(',', 508, 509)\n",
      "('nosse', 510, 515)\n",
      "('quod', 516, 520)\n",
      "('suscepi.', 521, 529)\n"
     ]
    }
   ],
   "source": [
    "docs = [('README', \"huius commentarii pertinebit fortassis et ad successorem utilitas,\"\n",
    "             \" sed cum inter initia administrationis meae scriptus sit,\"\n",
    "             \" in primis ad meam institutionem regulamque proficie\"),\n",
    "            ('LICENSE', \"Cum omnis res ab imperatore delegata intentiorem exigat curam,\"\n",
    "             \" et me seu naturalis sollicitudo seu fides sedula non ad\" \n",
    "             \" diligentiam modo verum ad amorem quoque commissae rei \"\n",
    "             \" instigent sitque nunc mihi ab Nerva Augusto, nescio diligentiore\"\n",
    "             \" an amantiore rei publicae imperatore, aquarum iniunctum officium\"\n",
    "             \" ad usum, tum ad salubritatem atque etiam securitatem urbis\"\n",
    "             \" pertinens, administratum per principes semper civitatis nostrae\"\n",
    "             \" viros, primum ac potissimum existimo,\" \n",
    "             \"sicut in ceteris negotiis institueram, nosse quod suscepi.\")\n",
    "          ]\n",
    "for item in latin_token.tokenize(docs[1][1]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_make_tokenizer():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    tokenizer_module = fts.make_tokenizer_module(WordTokenizer('latin'))\n",
    "    assert fts.tokenizer.sqlite3_tokenizer_module == type(tokenizer_module)\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_make_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_reginster_tokenizer():\n",
    "    name = 'simpe'\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    tokenizer_module = fts.make_tokenizer_module(WordTokenizer('latin'))\n",
    "    fts.register_tokenizer(c, name, tokenizer_module)\n",
    "    v = c.execute(\"SELECT FTS3_TOKENIZER(?)\", (name,)).fetchone()[0]\n",
    "    assert ctypes.addressof(tokenizer_module) == struct.unpack(\"P\", v)[0]\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "fts3tokenize disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0e086e91d5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_reginster_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-d55d5d997807>\u001b[0m in \u001b[0;36mtest_reginster_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':memory:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tokenizer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT FTS3_TOKENIZER(?)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgrant/projects/sqlite-fts-python/sqlitefts/tokenizer.py\u001b[0m in \u001b[0;36mregister_tokenizer\u001b[0;34m(c, name, tokenizer_module)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodule_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0maddress_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT fts3_tokenizer(?, ?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mtokenizer_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_addr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: fts3tokenize disabled"
     ]
    }
   ],
   "source": [
    "test_reginster_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_createtable():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    c.row_factory = sqlite3.Row\n",
    "    name = 'simple'\n",
    "    sql = \"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\".format(name)\n",
    "    fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "    c.execute(sql)\n",
    "    r = c.execute(\"SELECT * FROM sqlite_master WHERE type='table' AND name='fts'\").fetchone()\n",
    "    assert r\n",
    "    assert r[str('type')] == 'table' and r[str('name')] == 'fts' and r[str('tbl_name')] == 'fts'\n",
    "    assert r[str('sql')].upper() == sql.upper()\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "fts3tokenize disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f366a61c0ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_createtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-549ae43ab550>\u001b[0m in \u001b[0;36mtest_createtable\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'simple'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tokenizer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM sqlite_master WHERE type='table' AND name='fts'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgrant/projects/sqlite-fts-python/sqlitefts/tokenizer.py\u001b[0m in \u001b[0;36mregister_tokenizer\u001b[0;34m(c, name, tokenizer_module)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodule_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0maddress_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT fts3_tokenizer(?, ?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mtokenizer_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_addr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: fts3tokenize disabled"
     ]
    }
   ],
   "source": [
    "test_createtable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktLanguageVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_insert():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    c.row_factory = sqlite3.Row\n",
    "    name = 'simple'\n",
    "    content = 'Arma virumque cano, Troiae qui primus ab ori'\n",
    "    fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "    c.execute(\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\".format(name))\n",
    "    r = c.execute('INSERT INTO fts VALUES(?)', (content,))\n",
    "    assert r.rowcount == 1\n",
    "    r = c.execute(\"SELECT * FROM fts\").fetchone()\n",
    "    assert r\n",
    "    assert r[str('content')] == content\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "fts3tokenize disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8a552ac0e524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-5495ac4d5259>\u001b[0m in \u001b[0;36mtest_insert\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'simple'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Arma virumque cano, Troiae qui primus ab ori'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tokenizer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INSERT INTO fts VALUES(?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgrant/projects/sqlite-fts-python/sqlitefts/tokenizer.py\u001b[0m in \u001b[0;36mregister_tokenizer\u001b[0;34m(c, name, tokenizer_module)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodule_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0maddress_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT fts3_tokenizer(?, ?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mtokenizer_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_addr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: fts3tokenize disabled"
     ]
    }
   ],
   "source": [
    "test_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_match():\n",
    "    c = sqlite3.connect(':memory:')\n",
    "    c.row_factory = sqlite3.Row\n",
    "    name = 'simple'\n",
    "    contents = [('ibi linguam Atthidem primis pueritiae stipendiis merui',),\n",
    "                ('ibi linguam haec equidem ipsa vocis immutatio',)\n",
    "               ]\n",
    "    fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "    c.execute(\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\".format(name))\n",
    "    r = c.executemany('INSERT INTO fts VALUES(?)', contents)\n",
    "    assert r.rowcount == 2\n",
    "    r = c.execute(\"SELECT * FROM fts\").fetchall()\n",
    "    assert len(r) == 2\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'ibi'\").fetchall()\n",
    "    assert len(r) == 2\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'Atthidem'\").fetchall()\n",
    "    assert len(r) == 1 and r[0][str('content')] == contents[0][0]\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'haec'\").fetchall()\n",
    "    assert len(r) == 1 and r[0][str('content')] == contents[1][0]\n",
    "    r = c.execute(\"SELECT * FROM fts WHERE fts MATCH 'zzz'\").fetchall()\n",
    "    assert len(r) == 0\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "fts3tokenize disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-66706d1f70e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-8db39ea6767d>\u001b[0m in \u001b[0;36mtest_match\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'ibi linguam haec equidem ipsa vocis immutatio'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                ]\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tokenizer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE VIRTUAL TABLE fts USING FTS4(tokenize={})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INSERT INTO fts VALUES(?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgrant/projects/sqlite-fts-python/sqlitefts/tokenizer.py\u001b[0m in \u001b[0;36mregister_tokenizer\u001b[0;34m(c, name, tokenizer_module)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodule_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0maddress_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT fts3_tokenizer(?, ?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mtokenizer_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_addr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: fts3tokenize disabled"
     ]
    }
   ],
   "source": [
    "test_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_full_text_index_queries():\n",
    "    name = 'simple'\n",
    "    docs = [('README', \"huius commentarii pertinebit fortassis et ad successorem utilitas,\"\n",
    "             \" sed cum inter initia administrationis meae scriptus sit,\"\n",
    "             \" in primis ad meam institutionem regulamque proficie\"),\n",
    "            ('LICENSE', \"Cum omnis res ab imperatore delegata intentiorem exigat curam,\"\n",
    "             \" et me seu naturalis sollicitudo seu fides sedula non ad\" \n",
    "             \" diligentiam modo verum ad amorem quoque commissae rei \"\n",
    "             \" instigent sitque nunc mihi ab Nerva Augusto, nescio diligentiore\"\n",
    "             \" an amantiore rei publicae imperatore, aquarum iniunctum officium\"\n",
    "             \" ad usum, tum ad salubritatem atque etiam securitatem urbis\"\n",
    "             \" pertinens, administratum per principes semper civitatis nostrae\"\n",
    "             \" viros, primum ac potissimum existimo,\" \n",
    "             \"sicut in ceteris negotiis institueram, nosse quod suscepi.\")\n",
    "          ]\n",
    "    with sqlite3.connect(':memory:') as c:\n",
    "        c.row_factory = sqlite3.Row\n",
    "        fts.register_tokenizer(c, name, fts.make_tokenizer_module(WordTokenizer('latin')))\n",
    "        c.execute(\"CREATE VIRTUAL TABLE docs USING FTS4(title, body, tokenize={})\".format(name))\n",
    "        c.executemany(\"INSERT INTO docs(title, body) VALUES(?, ?)\", docs)\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'huius'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'sed'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'sed*'\").fetchall()\n",
    "        assert len(r) == 2\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'comm'\").fetchall()\n",
    "        assert len(r) == 0\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'commi*'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'comm*'\").fetchall()\n",
    "        assert len(r) == 2\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH 'quod suscepi'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "        r = c.execute(\"SELECT * FROM docs WHERE docs MATCH '\\\"qu* sus*\\\"'\").fetchall()\n",
    "        assert len(r) == 1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "fts3tokenize disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d73c5bcdf3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_full_text_index_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-5a1af2b52586>\u001b[0m in \u001b[0;36mtest_full_text_index_queries\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':memory:'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tokenizer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE VIRTUAL TABLE docs USING FTS4(title, body, tokenize={})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INSERT INTO docs(title, body) VALUES(?, ?)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgrant/projects/sqlite-fts-python/sqlitefts/tokenizer.py\u001b[0m in \u001b[0;36mregister_tokenizer\u001b[0;34m(c, name, tokenizer_module)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodule_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddressof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0maddress_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT fts3_tokenizer(?, ?)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mtokenizer_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_addr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: fts3tokenize disabled"
     ]
    }
   ],
   "source": [
    "test_full_text_index_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
